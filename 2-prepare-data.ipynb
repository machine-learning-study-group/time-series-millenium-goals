{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare Data\n",
    "\n",
    "From https://machinelearningmastery.com/process-for-working-through-machine-learning-problems/\n",
    "\n",
    "I preface data preparation with a data analysis phase that involves summarizing the attributes and visualizing them using scatter plots and histograms. I also like to describe in detail each attribute and relationships between attributes. This grunt work forces me to think about the data in the context of the problem before it is lost to the algorithms\n",
    "\n",
    "The actual data preparation process is three step as follows:\n",
    "\n",
    "* Step 1: Data Selection: Consider what data is available, what data is missing and what data can be removed.\n",
    "* Step 2: Data Preprocessing: Organize your selected data by formatting, cleaning and sampling from it.\n",
    "* Step 3: Data Transformation: Transform preprocessed data ready for machine learning by engineering features using scaling, attribute decomposition and attribute aggregation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expands column width to maximum\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve resources\n",
    "The data directory will be ignored by git and you will need to retrieve manually the resources:\n",
    "\n",
    "extract the contents of this zip file into the data subdirectory: \n",
    "https://s3.amazonaws.com/drivendata/data/1/public/cd238763-ed29-4a46-8584-f9334d57ec94.zip\n",
    "you should have \"data/TrainingSet.csv\" and \"data/SubmissionRows.csv\"\n",
    "\n",
    "\n",
    "you will need as well to put this file in the data/ folder: \n",
    "https://gist.githubusercontent.com/pamelafox/986163/raw/f5f9db4f1b287804fd07ffb3296ed0036292bc7a/countryinfo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/TrainingSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks at the first few lines to get an idea what the actual data looks like\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last few lines\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column looks like a row ID, let's double check that it's the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,0].nunique() == df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are as many rows in the dataframe as identifiers in the first column, let's reload the data using the first column as the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/TrainingSet.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What column names do we have\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rename the columns to remove gaps and unnecessary info and to ease further coding when selecting sets of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [year[:4] for year in df.columns][:-3] + [col.replace(' ', '_') for col in df.columns.values[-3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some basic stats about the training data\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique countries\n",
    "df.Country_Name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Number of distinct countries {}\".format(df.Country_Name.nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas can also tell us how many unique values are in each column\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or the percentage of non-nul data in each column\n",
    "(df.count() / df.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data containing the rows we need to predict\n",
    "df_submission = pd.read_csv('data/SubmissionRows.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have the row ID as an index for both the training data (df) and the submission we can directly extract the data related to the submission index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_in_data = df.loc[df_submission.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_in_data.Country_Name.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the Series Code in the sudmission data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_codes = df_submission_in_data.Series_Code.unique()\n",
    "len(submission_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Series_Code.isin(submission_codes).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or the percentage of non-nul data in each column\n",
    "(df.count() / df.shape[0] * 100).iloc[:-4].plot(label='training')\n",
    "(df_submission_in_data.count() / df_submission_in_data.shape[0] * 100).iloc[:-4].plot(label='submission')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have learned from the analysis of the submission data that:\n",
    "1. Most countries are represented in the submission data, 206 out of 214\n",
    "2. Only 7 codes are used and we have 1118 data points in the training data with these codes\n",
    "3. The amount of missing data is fairly similar between the training and submission data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are we trying to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 737 indicators from 206 countries with data from 1972 to 2007.  \n",
    "We would like to predict what these indicators will be in 2008 and 2012.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very simplistic way of predicting the future values of these indicators would be to do a simple linear regression for indicators with more than 1 data point in the last 35 years or use the only data point we have for indicators with a single value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try to code this simplistic version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(row):\n",
    "    data = row.loc['1972':'2007']\n",
    "    nbr_data_points = data.count()\n",
    "    if nbr_data_points < 2:\n",
    "        pred_2008 = data.dropna().values\n",
    "        pred_2012 = pred_2008\n",
    "    \n",
    "    else:\n",
    "        years = data.dropna().index.values.astype(np.int).reshape(-1, 1)\n",
    "        values = data.dropna().values\n",
    "        \n",
    "        #linear regression\n",
    "        regr = LinearRegression()\n",
    "        regr.fit(years, values)\n",
    "        \n",
    "        #predictions\n",
    "        pred_2008 = regr.predict(np.array([2008]).reshape(-1, 1))\n",
    "        pred_2012 = regr.predict(np.array([2012]).reshape(-1, 1))\n",
    "        \n",
    "    return pred_2008[0], pred_2012[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simple_preds = pd.DataFrame(df_submission_in_data.apply(make_prediction, axis=1).tolist(), index=df_submission_in_data.index, columns=['2008','2012'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simple_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(df_train, df_pred, nbr_rows):\n",
    "    rows_to_plot = np.random.choice(df_train.index.values, nbr_rows, replace=False)\n",
    "    \n",
    "    cmap = get_cmap('Set1')\n",
    "    colors = cmap.colors\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(12,10))\n",
    "    for i,j in zip(rows_to_plot, range(nbr_rows)):\n",
    "        if j >= len(colors): j -= len(colors)\n",
    "        ax.plot(df_train.loc[i, '1972':'2007'].dropna().index.astype(int), \n",
    "                df_train.loc[i, '1972':'2007'].dropna().values, \n",
    "                label=df_train.loc[i, 'Country_Name']+ '/' + df_train.loc[i, 'Series_Name'],\n",
    "                marker='o',\n",
    "                linewidth=4,\n",
    "                alpha=0.5,\n",
    "                color=colors[j])\n",
    "                \n",
    "        ax.plot(df_pred.loc[i].index.astype(int), \n",
    "                df_pred.loc[i].values,\n",
    "                marker='s',\n",
    "                linewidth=4,\n",
    "                markersize=10,\n",
    "                color=colors[j])\n",
    "\n",
    "    plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(df_submission_in_data, df_simple_preds, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are simplistic but not too bad for most indicators.  \n",
    "We could improve easily the results by doing a polynomial regression or another type of more sophisticated regression.  \n",
    "\n",
    "But what we are trying to achieve is find correlation between these indicators and all the other indicators present in the dataset to improve our predictions as we have a lot of missing data.  \n",
    "And these predictions using correlations are where the difficulty of this exercise lies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating sub codes from the main code values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first make all values upper case to make sure we compare correctly the codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Series_Code = df.Series_Code.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Series_Code.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1305 different code names but most of them include similar codes which might be helpful when trying to find similarity within the data to help with our predictions.  \n",
    "We will create a function that extract each of the subcodes and save them in separate column in function of their order in the code serie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the code structure\n",
    "df.Series_Code.apply(lambda x: len(x.split('.'))).plot(kind='hist');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_longest_code_serie = df.Series_Code.apply(lambda x: len(x.split('.'))).idxmax()\n",
    "df.loc[row_longest_code_serie, 'Series_Code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.loc[row_longest_code_serie, 'Series_Code'].split('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a maximum of 7 separate elements in the code structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_code(s):\n",
    "    # split the code name using the '.'\n",
    "    s_split = s.split('.')\n",
    "    \n",
    "    # save the first and last value of the serie\n",
    "    last = s_split[-1]\n",
    "    first = s_split[0]\n",
    "    \n",
    "    # reverse the order of the code series to keep the last code as first and first as last\n",
    "    #s_split = s_split[::-1]\n",
    "    \n",
    "    # add NaN values until the list has 7 items in it\n",
    "    s_split += [np.NaN] * (7 - len(s_split))\n",
    "    \n",
    "    # add the first and last value to the list\n",
    "    s_split += [first, last]\n",
    "    \n",
    "    return s_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first create 9 new columns for each of the output of the function explode_code\n",
    "# fill these columns with the ouput of the explode_code function by converting its ouput to a list and then to a dataframe using the orginal index of the dataframe\n",
    "\n",
    "df[['Series_Code_'+ str(i) for i in range(7)] + ['Series_Code_First', 'Series_Code_Last']] =\\\n",
    "    pd.DataFrame(df.Series_Code.apply(explode_code).tolist(), index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:5,-11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[-5:,-11:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the continent related to each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import countryinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_continent = pd.DataFrame(countryinfo.countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_continent.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We copy the continent information from the dataframe using the country names as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the map function of pandas for looking the the country name in the index of the second df and outputing the continent information\n",
    "df['Continent'] = df.Country_Name.map(df_continent.set_index('name').continent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Continent','Country_Name']].iloc[::15000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some countries with names that are different from our list of country/continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Continent.isna()].Country_Name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Country_Name.str.contains('Bahamas', case=False)].Country_Name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_continent[df_continent.name.str.contains('Bahamas', case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continent missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some fuzzy matching to find the most likely candidate for the country listed in our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_bad_name = df[df.Continent.isna()].Country_Name.unique()\n",
    "countries = df_continent.name.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = country_bad_name[0]\n",
    "print(country)\n",
    "process.extractOne(country, countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fuzzy = pd.DataFrame([[country]+list(process.extractOne(country, countries)) for country in country_bad_name], \n",
    "                        columns=['training_name', 'replacement_name', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fuzzy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the continent to this dataframe\n",
    "df_fuzzy['Continent'] = df_fuzzy.replacement_name.map(df_continent.set_index('name').continent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fuzzy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacement_name that we are most sure about\n",
    "df_fuzzy[df_fuzzy.score > 86]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacement_name that we are most sure about\n",
    "df_fuzzy[df_fuzzy.score < 86]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are missing a few easy fix like Congo and Korea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill the missing continents using the above rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_to_correct = df.Country_Name.isin(df_fuzzy[df_fuzzy.score > 86].training_name)\n",
    "\n",
    "df.loc[countries_to_correct, 'Continent'] =\\\n",
    "    df.loc[countries_to_correct, 'Country_Name'].map(\n",
    "        df_fuzzy[df_fuzzy.score > 86].set_index('training_name').Continent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's redo the above fuzzy matching but trying a different metric to catch more missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_bad_name = df[df.Continent.isna()].Country_Name.unique()\n",
    "countries = df_continent.name.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = country_bad_name[5]\n",
    "print(country)\n",
    "print(process.extractOne(country, countries))\n",
    "print(process.extractOne(country, countries, scorer=fuzz.ratio))\n",
    "print(process.extractOne(country, countries, scorer=fuzz.partial_ratio))\n",
    "print(process.extractOne(country, countries, scorer=fuzz.token_sort_ratio))\n",
    "print(process.extractOne(country, countries, scorer=fuzz.token_set_ratio))\n",
    "print(process.extractOne(country, countries, scorer=fuzz.partial_token_set_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fuzzy = pd.DataFrame([[country]+list(process.extractOne(country, \n",
    "                                                           countries, \n",
    "                                                           scorer=fuzz.ratio\n",
    "                                                          )) for country in country_bad_name], \n",
    "                        columns=['training_name', 'replacement_name', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the continent to this dataframe\n",
    "df_fuzzy['Continent'] = df_fuzzy.replacement_name.map(df_continent.set_index('name').continent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacement_name that we are most sure about\n",
    "df_fuzzy[df_fuzzy.score > 79]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill the missing continents using the above rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_to_correct = df.Country_Name.isin(df_fuzzy[df_fuzzy.score > 79].training_name)\n",
    "\n",
    "df.loc[countries_to_correct, 'Continent'] =\\\n",
    "    df.loc[countries_to_correct, 'Country_Name'].map(\n",
    "        df_fuzzy[df_fuzzy.score > 79].set_index('training_name').Continent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual corrections for Korea, Congo and China"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_continent[df_continent.name.str.contains('congo', case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_continent[df_continent.name.str.contains('korea', case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.Country_Name.str.contains('congo', case=False), 'Continent'] = 'Africa'\n",
    "df.loc[df.Country_Name.str.contains('korea', case=False), 'Continent'] = 'Asia'\n",
    "df.loc[df.Country_Name.str.contains('china', case=False), 'Continent'] = 'Asia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is left in our missing continent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_bad_name = df[df.Continent.isna()].Country_Name.unique()\n",
    "country_bad_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(country_bad_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Country_Name.isin(country_bad_name).sum() / df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_in_data.Country_Name.isin(country_bad_name).sum() / df_submission_in_data.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have less than 5% of the data with a missing continent in the training and submission data so we can probably leave it as it is and just replace it with Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.Country_Name.isin(country_bad_name), 'Continent'] = 'Missing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the subcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 most common subcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(df.loc[:, 'Series_Code_0':'Series_Code_Last'].values.flat).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common codes for each subcode column ignoring the nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_code(array, codes):\n",
    "    array_notna = array[~pd.isna(array)]\n",
    "    common_code = np.array(Counter(array_notna).most_common(codes))[:,0]\n",
    "    common_code = np.append(common_code, [np.nan]*(codes-len(common_code)))\n",
    "    return common_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data= np.apply_along_axis(func1d=most_common_code, \n",
    "                                       arr=df.loc[:, 'Series_Code_0':'Series_Code_Last'].values,\n",
    "                                       axis=0,\n",
    "                                      codes=15), \n",
    "             columns=df.loc[:, 'Series_Code_0':'Series_Code_Last'].columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
